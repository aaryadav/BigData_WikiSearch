{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# DAANISH ARA YAKTA\n",
        "# PUSHPENDER SHARMA\n",
        "# AARYAMAN YADAV"
      ],
      "metadata": {
        "id": "gUN4bK-hyA50"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install wikiextractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03QZvicoxGJn",
        "outputId": "b4992ded-5899-42c2-fd35-0d43ab47276b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 36 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 54.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=9159efafba6e95ce7b05647d12ccd9e18e169921e11dd93103558f60f0d61487\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wikiextractor in /usr/local/lib/python3.7/dist-packages (3.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n",
        "!wikiextractor /content/enwiki-latest-pages-articles.xml.bz2"
      ],
      "metadata": {
        "id": "aeJ_LOiby7-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "GT-NLnOPtbHn"
      },
      "outputs": [],
      "source": [
        "from operator import add\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "aaryaman_conf = SparkConf()\\\n",
        "        .setAppName(\"Wiki_Search_Engine\")\\\n",
        "        .setMaster(\"local[*]\")\\\n",
        "        .set(\"spark.driver.memory\", \"10g\")\\\n",
        "        .set(\"spark.driver.maxResultSize\", \"4g\")\n",
        "\n",
        "pushpender_sc = SparkContext(conf=aaryaman_conf)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rawData = sc.textFile(r\"wiki\\artile_per_line.txt\")\n",
        "flatten_words = rawData.flatMap(lambda x: x.split('\\t')[1].split())\n",
        "words_joint = flatten_words.map(lambda x: (x, 1)).reduceByKey(add)\n",
        "print \"Number of distinct words in the whole Wikipedia: \", words_joint.count()\n",
        "# Number of distinct words in the whole Wikipedia:  26764007"
      ],
      "metadata": {
        "id": "wvT2GvBhzJOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9BHy6afYtbHz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from spacy.en import English\n",
        "import nltk\n",
        "import spacy.attrs\n",
        "import time\n",
        "import codecs\n",
        "\n",
        "fo = open(r'wiki\\titles.txt')\n",
        "raw_text = fo.read()\n",
        "fo.close()\n",
        "\n",
        "wiki_title_words = set(raw_text.split())\n",
        "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "NLP = English()\n",
        "\n",
        "fo = codecs.open(r'wiki\\artile_per_line.txt', 'r', encoding='utf-8') # title \\t document\n",
        "out = codecs.open(r\"Wiki\\article_lemma.txt\", 'w', encoding='utf-8')\n",
        "\n",
        "\n",
        "for doc in NLP.pipe(fo, n_threads=4):    \n",
        "    title_words = []\n",
        "    passed_title = False\n",
        "    for candidate in doc:\n",
        "        if '\\t' not in candidate.lemma_: # title \\t document\n",
        "            if passed_title:\n",
        "                if (candidate.lemma_ not in stop_words) and (candidate.pos_ != u'PUNCT') and \\\n",
        "                    (candidate.lemma_ in english_vocab or candidate.lemma_ in wiki_title_words):\n",
        "\n",
        "                    out.write(candidate.lemma_ + u' ')\n",
        "            else:\n",
        "                title_words.append(candidate.orth_)\n",
        "        else:\n",
        "            out.write(u''.join(title_words) + candidate.orth_)\n",
        "            passed_title = True\n",
        "\n",
        "    out.write('\\n')\n",
        "\n",
        "fo.close()\n",
        "out.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1axEA6UtbH3"
      },
      "source": [
        "Now let's run the word count in Spark and see how many words we have left now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xSpRUgI7tbH6"
      },
      "outputs": [],
      "source": [
        "from operator import add\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf()\\\n",
        "        .setAppName(\"Wiki_Search_Engine\")\\\n",
        "        .setMaster(\"local[*]\")\\\n",
        "        .set(\"spark.driver.memory\", \"10g\")\\\n",
        "        .set(\"spark.driver.maxResultSize\", \"4g\")\n",
        "\n",
        "sc = SparkContext(conf=conf)\n",
        "rawData = sc.textFile(r\"wiki\\article_lemma.txt\")\n",
        "flatten_words = rawData.flatMap(lambda x: x.split('\\t')[1].split())\n",
        "words_joint = flatten_words.map(lambda x: (x, 1)).reduceByKey(add)\n",
        "print \"Number of distinct lemmas in the whole Wikipedia: \", words_joint.count()\n",
        "# Number of distinct lemmas in the whole Wikipedia:  561354"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gM_d8FotbH8"
      },
      "source": [
        "Number of distinct words left in the documents after pre-processing is 561354. That means more than 98% of the words from the original documents are all removed. This means we can comfortably hash the remaining words to build our tf-idf model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pQ66HyPntbH-"
      },
      "outputs": [],
      "source": [
        "sc = SparkContext(conf=conf)\n",
        "rawData = sc.textFile(r\"wiki\\articles_lemma.txt\")\n",
        "documents = rawData.map(lambda line : line.split('\\t')[1].split())\n",
        "titles = rawData.map(lambda line : line.split('\\t')[0])\n",
        "titles.cache()\n",
        "\n",
        "hashingTF = HashingTF(20000000)  #20 Million hash buckets just to make sure it fits in memory\n",
        "tf = hashingTF.transform(documents)\n",
        "idf = IDF(minDocFreq=10).fit(tf)\n",
        "tfidf = idf.transform(tf)\n",
        "tfidf.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffhSAtv8tbH-"
      },
      "source": [
        "Now that we have tf-idf model for all documents. We can send queries and return the documents that have the highest score for that perticular word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4mEpkNKtbH_",
        "outputId": "1b025c09-86df-49b7-bc8c-07e2445ec576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 related documents:\n",
            "Tabriz\n",
            "Azerbaijan(Iran)\n",
            "Safina-yiTabriz\n",
            "Tabrizrug\n",
            "OldAzerilanguage\n",
            "TabrizKhanate\n",
            "Persiancarpet\n",
            "Safavidart\n",
            "MachineSaziTabrizF.C.\n",
            "EastAzerbaijanProvince\n"
          ]
        }
      ],
      "source": [
        "QueryTF = hashingTF.transform([\"tabriz\"])\n",
        "QueryHashValue = QueryTF.indices[0]\n",
        "QueryRelevance = tfidf.map(lambda x: x[QueryHashValue])\n",
        "zippedResults = QueryRelevance.zip(titles)\n",
        "print \"Top 10 related documents:\"\n",
        "for (k, v) in zippedResults.sortByKey(ascending=False).take(10):\n",
        "    print v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMXx3e27tbIB"
      },
      "source": [
        "As you can see it works! Tabriz is a city in Iran and all the returned results are related to the word with the first link to the Wikipedia page of the city. I run this on my local machine which takes a minute or two for each query but if we distribute this on a cluster it should return the results much faster. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}